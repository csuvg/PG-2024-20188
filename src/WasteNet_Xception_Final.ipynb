{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAc9MmhL2rw"
      },
      "source": [
        "# Proyecto: Clasificación de Desechos Sólidos utilizando Xception (Final)\n",
        "\n",
        "# Nombre: Pedro Arriola\n",
        "# Carnet: 20188\n",
        "---\n",
        "\n",
        "## 1. Introducción\n",
        "\n",
        "En este proyecto, nuestro objetivo es crear un modelo de **clasificación de desechos sólidos** utilizando la arquitectura **Xception**, una red neuronal convolucional profunda preentrenada en el conjunto de datos **ImageNet**. La idea principal es construir un modelo capaz de clasificar imágenes de desechos sólidos en diferentes categorías como `battery`, `glass`, `metal`, `organic`, `paper` y `plastic`.\n",
        "\n",
        "El problema de la clasificación de desechos sólidos es crucial para el manejo de residuos y el reciclaje eficiente. Un sistema automatizado que pueda clasificar correctamente diferentes tipos de desechos puede ayudar a mejorar la eficiencia en los procesos de reciclaje y reducir el impacto ambiental.\n",
        "\n",
        "Este proyecto aborda las siguientes etapas:\n",
        "1. Uso del modelo **Xception** preentrenado.\n",
        "2. Implementación de técnicas de **regularización** (L2) y **dropout** para mejorar la generalización.\n",
        "3. Realización de **fine-tuning** en el modelo para ajustar sus capas superiores y mejorar su rendimiento.\n",
        "4. Evaluación de los resultados del modelo utilizando métricas como **precision**, **recall** y **f1-score**, además de una **matriz de confusión**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. ¿Qué es el modelo Xception?\n",
        "\n",
        "**Xception** es un modelo convolucional profundo basado en una arquitectura de redes neuronales que se destaca por su uso eficiente de convoluciones separables en profundidad (depthwise separable convolutions). Fue desarrollado como una mejora del modelo Inception y se ha demostrado que ofrece un rendimiento superior en tareas de clasificación de imágenes.\n",
        "\n",
        "La arquitectura de Xception se caracteriza por:\n",
        "- **Convoluciones separables en profundidad**: Estas convoluciones descomponen una convolución estándar en dos operaciones: una convolución depthwise seguida de una convolución pointwise. Esto reduce el número de parámetros y mejora la eficiencia computacional.\n",
        "- **Capas profundas**: Xception tiene muchas capas de convolución, lo que le permite aprender características complejas de las imágenes.\n",
        "- **Preentrenamiento en ImageNet**: Xception está preentrenado en el conjunto de datos **ImageNet**, lo que significa que ya ha aprendido a reconocer características visuales generales como bordes, texturas y formas, lo que facilita su adaptación a nuevas tareas mediante el proceso de **transfer learning**.\n",
        "\n",
        "En este proyecto, aprovechamos este modelo preentrenado y ajustamos sus capas superiores para adaptarlo a la tarea específica de clasificación de desechos sólidos.\n",
        "---\n",
        "## 3. Preprocesamiento de las Imágenes\n",
        "\n",
        "El preprocesamiento de las imágenes es una etapa crucial en cualquier tarea de clasificación de imágenes, especialmente cuando se utilizan modelos preentrenados como **Xception**. Cada modelo preentrenado tiene su propia función de preprocesamiento específica que ajusta las imágenes para que coincidan con los datos con los que se entrenó el modelo original.\n",
        "\n",
        "### 3.1 Preprocesamiento en el Modelo Xception\n",
        "\n",
        "Para el modelo **Xception**, se utiliza la función de preprocesamiento `preprocess_input`. Esta función realiza varias transformaciones importantes en las imágenes antes de alimentarlas al modelo:\n",
        "\n",
        "- **Escalado**: Las imágenes se escalan para que sus valores estén en el rango de [-1, 1] (en lugar del rango estándar de [0, 255] para los valores de píxeles). Este escalado específico coincide con los datos utilizados durante el preentrenamiento de Xception en ImageNet.\n",
        "- **Normalización**: La función `preprocess_input` normaliza los valores de los píxeles, ayudando al modelo a tener entradas más estables, lo que a su vez mejora la convergencia durante el entrenamiento.\n",
        "\n",
        "### 3.2 Ajuste del Tamaño de las Imágenes\n",
        "\n",
        "Otra transformación importante fue ajustar las imágenes al tamaño requerido por el modelo **Xception**, que es **299x299 píxeles**. Las redes convolucionales preentrenadas como Xception requieren que las imágenes de entrada tengan un tamaño específico para que coincidan con la estructura de la red.\n",
        "\n",
        "### 3.3 Generación de Imágenes\n",
        "\n",
        "Para el preprocesamiento y la generación de imágenes durante el entrenamiento, se utilizó la clase `ImageDataGenerator` de Keras. Esta clase permite aplicar transformaciones como el escalado y facilita el procesamiento eficiente de las imágenes en lotes, lo que permite entrenar el modelo en grandes conjuntos de datos. Además, es posible aplicar **data augmentation** utilizando esta clase para generar más variabilidad en los datos, mejorando así la capacidad del modelo para generalizar.\n",
        "\n",
        "### 3.4 Explicación del Preprocesamiento\n",
        "\n",
        "1. **Preprocesamiento Específico de Xception**:\n",
        "   - La función `preprocess_input` escala los valores de los píxeles a un rango de [-1, 1], que es el formato esperado por el modelo Xception.\n",
        "   - Esto asegura que el modelo reciba las imágenes en un formato compatible con lo que se utilizó durante su preentrenamiento en ImageNet, lo que mejora el rendimiento del modelo.\n",
        "\n",
        "2. **Data Augmentation (Opcional)**:\n",
        "   - Además del preprocesamiento básico, también se puede aplicar **data augmentation** para mejorar la capacidad del modelo de generalizar a imágenes nuevas.\n",
        "   - Algunas técnicas comunes de data augmentation incluyen rotaciones, cambios de escala, ajustes de brillo y traslaciones. Estas técnicas pueden aplicarse a las imágenes durante el entrenamiento para aumentar la variabilidad del conjunto de datos.\n",
        "\n",
        "### 3.5 Importancia del Preprocesamiento\n",
        "\n",
        "El preprocesamiento adecuado es esencial para garantizar que las imágenes se presenten al modelo en el formato correcto. Dado que **Xception** fue entrenado en ImageNet con imágenes preprocesadas de una manera específica, replicar este preprocesamiento es clave para aprovechar al máximo las capacidades del modelo preentrenado.\n",
        "\n",
        "En resumen, para el modelo **Xception**, el preprocesamiento incluyó:\n",
        "- Escalado de las imágenes al rango [-1, 1] utilizando la función de preprocesamiento adecuada.\n",
        "- Ajuste de las imágenes al tamaño de **299x299 píxeles** para cumplir con el formato esperado por el modelo.\n",
        "- Generación de las imágenes de entrenamiento y validación utilizando técnicas de preprocesamiento eficientes, con la posibilidad de aplicar **data augmentation** para aumentar la variabilidad y robustez del conjunto de datos.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Construcción del Modelo\n",
        "\n",
        "El modelo que construimos utiliza **Xception** como base preentrenada, con capas adicionales densas para clasificar las imágenes en nuestras 6 clases de desechos sólidos. Como parte del proceso de construcción del modelo, se congelaron las capas inferiores de Xception para conservar las características previamente aprendidas y se añadieron capas densas y de regularización.\n",
        "\n",
        "### Detalles de la Construcción del Modelo:\n",
        "\n",
        "1. **Base del modelo (Xception)**: Cargamos el modelo preentrenado Xception sin las capas superiores (`include_top=False`). Esto nos permite utilizar las características visuales aprendidas previamente en el conjunto de datos ImageNet.\n",
        "   \n",
        "2. **Capas densas adicionales**:\n",
        "   - Se añadieron dos capas densas: la primera con **1024 unidades** y la segunda con **512 unidades**, ambas activadas con la función `ReLU`.\n",
        "   - Para evitar el sobreajuste (*overfitting*), se aplicó **regularización L2** a ambas capas densas, con un coeficiente de regularización de **0.002**.\n",
        "   - Se utilizó **dropout** con una tasa del **60%** para reducir la posibilidad de que el modelo dependa demasiado de ciertas características durante el entrenamiento.\n",
        "\n",
        "3. **Capa de salida**: Se añadió una capa de salida con **6 unidades** (una para cada clase) activada con la función `softmax`, lo que permite clasificar las imágenes en una de las seis categorías de desechos sólidos.\n",
        "\n",
        "### Hiperparámetros utilizados:\n",
        "\n",
        "- **Batch size**: 64\n",
        "- **Epochs**: 200 (con early stopping)\n",
        "- **Regularización L2**: 0.002\n",
        "- **Dropout**: 0.6\n",
        "- **Optimización**: Adam con `learning_rate=0.0001`\n",
        "- **Callbacks**: EarlyStopping y ReduceLROnPlateau\n",
        "\n",
        "Se implementaron los siguientes *callbacks*:\n",
        "- **EarlyStopping**: para detener el entrenamiento si no se veía mejora en la pérdida de validación durante 15 épocas.\n",
        "- **ReduceLROnPlateau**: para reducir la tasa de aprendizaje si no había mejora en la pérdida de validación tras 5 épocas.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Fine-tuning del Modelo\n",
        "\n",
        "Después de entrenar el modelo con las capas de Xception congeladas, realizamos un proceso de **fine-tuning** para ajustar las capas superiores del modelo base. Esto permite que el modelo ajuste mejor las características aprendidas específicamente a la tarea de clasificación de desechos sólidos.\n",
        "\n",
        "El fine-tuning se realizó descongelando las últimas **50 capas** del modelo Xception, permitiendo que estas capas se ajusten durante el proceso de entrenamiento con un *learning rate* más bajo (`1e-5`).\n",
        "\n",
        "----\n",
        "## 6. Resultados Obtenidos\n",
        "\n",
        "Después de aplicar **regularización L2** y **fine-tuning**, obtuvimos los siguientes resultados en el conjunto de validación:\n",
        "\n",
        "| Clase     | Precision | Recall  | F1-Score | Soporte |\n",
        "|-----------|-----------|---------|----------|---------|\n",
        "| battery   | 1.00      | 0.99    | 0.99     | 87      |\n",
        "| glass     | 0.87      | 0.93    | 0.90     | 73      |\n",
        "| metal     | 0.94      | 0.96    | 0.95     | 69      |\n",
        "| organic   | 0.97      | 1.00    | 0.98     | 64      |\n",
        "| paper     | 0.98      | 0.99    | 0.98     | 92      |\n",
        "| plastic   | 0.93      | 0.84    | 0.88     | 80      |\n",
        "\n",
        "**Precisión Global**: 0.95\n",
        "\n",
        "**Macro Promedio**:\n",
        "- **Precision**: 0.95\n",
        "- **Recall**: 0.95\n",
        "- **F1-Score**: 0.95\n",
        "\n",
        "**Promedio Ponderado**:\n",
        "- **Precision**: 0.95\n",
        "- **Recall**: 0.95\n",
        "- **F1-Score**: 0.95\n",
        "\n",
        "### Observaciones:\n",
        "- **Precisión global del 95%**: El modelo ha logrado una precisión global alta, mostrando que es capaz de generalizar bien a datos no vistos.\n",
        "- **Clases con mejor rendimiento**: Las clases \"battery\", \"organic\" y \"paper\" tienen un rendimiento excelente, con *f1-scores* cercanos al 0.99.\n",
        "- **Clases más difíciles**: La clase \"plastic\" mostró un *recall* ligeramente más bajo (84%), lo que sugiere que el modelo tiene alguna dificultad para distinguir correctamente algunas imágenes de plástico.\n",
        "\n",
        "----\n",
        "\n",
        "## 7. Conclusiones\n",
        "\n",
        "- El uso de la arquitectura **Xception** junto con **regularización L2** y **dropout** ha permitido obtener un **alto rendimiento** en la clasificación de desechos sólidos, con una precisión global del **95%**.\n",
        "  \n",
        "- El **fine-tuning** de las capas superiores del modelo base permitió ajustar las características aprendidas a la tarea específica, mejorando aún más el rendimiento del modelo.\n",
        "\n",
        "- Aunque el rendimiento general es excelente, la clase \"plastic\" aún muestra cierto grado de confusión con un *recall* del 84%. Para mejorar esta clase, podría ser útil aplicar **data augmentation** adicional (rotaciones, zoom, cambios de brillo, etc.) o ajustar más finamente el modelo con un mayor enfoque en esta clase.\n",
        "\n",
        "- El modelo es suficientemente robusto para su uso en aplicaciones de clasificación de desechos sólidos, lo que puede contribuir a mejorar los procesos de reciclaje y gestión de residuos de manera más eficiente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v08MXExBqMYs"
      },
      "outputs": [],
      "source": [
        "!pip install -q seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETm9lPE55Uve"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0EQituCoJ8"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDdoR0KUC_5u"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/Colab_Notebooks/EcoScan/trash'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfbMwZT7DA9K"
      },
      "outputs": [],
      "source": [
        "# Revisa si el path existe y recopila información de las clases\n",
        "if os.path.exists(data_path):\n",
        "    # Obtiene los nombres de las clases y cuenta las imágenes en cada clase\n",
        "    class_names = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
        "    class_counts = {class_name: len(os.listdir(os.path.join(data_path, class_name))) for class_name in class_names}\n",
        "    total_images = sum(class_counts.values())\n",
        "\n",
        "    # Información resumen\n",
        "    print(\"Resumen del conjunto de datos:\")\n",
        "    print(f\"Total de clases: {len(class_names)}\")\n",
        "    print(f\"Total de imágenes: {total_images}\")\n",
        "    print(\"Distribución de clases:\")\n",
        "    for class_name, count in class_counts.items():\n",
        "        print(f\"  - {class_name}: {count} imágenes\")\n",
        "else:\n",
        "    print(\"La ruta de datos no existe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhz7iSIYCqZ2"
      },
      "source": [
        "## MODEL CONSTRUCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka3g62Wg5XMQ"
      },
      "outputs": [],
      "source": [
        "# Parámetros\n",
        "BATCH_SIZE = 32\n",
        "n_classes = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnR5ujXs5iuQ"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo base preentrenado Xception\n",
        "conv_base = Xception(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(299, 299, 3)  # Tamaño ajustado a 299x299 para Xception\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Congelar todas las capas de Xception excepto las dos últimas\n",
        "for layer in conv_base.layers:\n",
        "    layer.trainable = False  # Congelar todas las capas inicialmente"
      ],
      "metadata": {
        "id": "GXfX_YQQRE7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YbfweMX5jdd"
      },
      "outputs": [],
      "source": [
        "# Congelar todas las capas excepto las dos últimas\n",
        "for layer in conv_base.layers[:-2]:\n",
        "    layer.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7GNxKag5p15"
      },
      "outputs": [],
      "source": [
        "# Valor de la regularización L2 (puedes ajustarlo)\n",
        "l2_value = 0.002\n",
        "\n",
        "# Construir el modelo superior (top model) con regularización L2\n",
        "top_model = conv_base.output\n",
        "top_model = Flatten(name=\"flatten\")(top_model)\n",
        "top_model = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(l2_value))(top_model)  # Regularización L2\n",
        "top_model = BatchNormalization()(top_model)\n",
        "top_model = Dropout(0.6)(top_model)\n",
        "top_model = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_value))(top_model)  # Regularización L2\n",
        "top_model = BatchNormalization()(top_model)\n",
        "top_model = Dropout(0.6)(top_model)\n",
        "output_layer = Dense(n_classes, activation='softmax')(top_model)  # Capa de salida\n",
        "\n",
        "# Modelo final\n",
        "model = Model(inputs=conv_base.input, outputs=output_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_vp-Ven5rBZ"
      },
      "outputs": [],
      "source": [
        "# Función para mostrar imágenes aleatorias del dataset\n",
        "def plot_random_images(generator, num_images=20):\n",
        "    images, labels = next(generator)\n",
        "    random_indices = np.random.choice(images.shape[0], num_images, replace=False)\n",
        "    fig, axs = plt.subplots(4, 5, figsize=(15, 12))\n",
        "    fig.suptitle('Random 20 Images from the Generator', fontsize=16)\n",
        "    for i, ax in enumerate(axs.flatten()):\n",
        "        index = random_indices[i]\n",
        "        image = images[index]\n",
        "        label = labels[index]\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(f'Class: {np.argmax(label)}')\n",
        "        ax.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VWzkPWg5vv6"
      },
      "outputs": [],
      "source": [
        "# Preparar los generadores de datos\n",
        "original_data = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "    data_path, target_size=(299, 299), batch_size=BATCH_SIZE, class_mode=\"categorical\")\n",
        "plot_random_images(original_data)\n",
        "\n",
        "gen_train = ImageDataGenerator(preprocessing_function=preprocess_input)  # Preprocesamiento para Xception\n",
        "full_data = gen_train.flow_from_directory(data_path, target_size=(299, 299), batch_size=BATCH_SIZE, class_mode=\"categorical\")\n",
        "plot_random_images(full_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdr-ZFUv5wlL"
      },
      "outputs": [],
      "source": [
        "# Extraer nombres de archivos y etiquetas\n",
        "filenames = full_data.filenames\n",
        "labels = full_data.labels\n",
        "class_mapping = {value: str(key) for key, value in full_data.class_indices.items()}\n",
        "labels = [class_mapping[label] for label in labels]\n",
        "\n",
        "# Dividir en entrenamiento y validación\n",
        "train_filenames, test_filenames, train_labels, test_labels = train_test_split(\n",
        "    filenames, labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Crear DataFrames para entrenamiento y validación\n",
        "train_df = pd.DataFrame({'filename': train_filenames, 'class': train_labels})\n",
        "test_df = pd.DataFrame({'filename': test_filenames, 'class': test_labels})\n",
        "\n",
        "# Crear generadores de entrenamiento y validación\n",
        "train_data = gen_train.flow_from_dataframe(\n",
        "    train_df, directory=data_path, target_size=(299, 299),\n",
        "    batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=True, seed=42)\n",
        "\n",
        "test_data = gen_train.flow_from_dataframe(\n",
        "    test_df, directory=data_path, target_size=(299, 299),\n",
        "    batch_size=BATCH_SIZE, class_mode=\"categorical\", shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4atrZnH51oJ"
      },
      "outputs": [],
      "source": [
        "# Definir parámetros del entrenamiento\n",
        "num_epochs = 200\n",
        "opt = Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKk-Shpj533x"
      },
      "outputs": [],
      "source": [
        "# Resumen del modelo\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYu0vUcx56mY"
      },
      "outputs": [],
      "source": [
        "# Definir pasos por época\n",
        "n_steps = train_data.samples // BATCH_SIZE\n",
        "n_val_steps = test_data.samples // BATCH_SIZE\n",
        "n_steps, n_val_steps\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kig-IIGJ59kO"
      },
      "outputs": [],
      "source": [
        "from tf_explain.callbacks.grad_cam import GradCAMCallback\n",
        "\n",
        "# Especificar el directorio de salida para guardar las visualizaciones\n",
        "output_dir = \"/content/drive/MyDrive/Colab_Notebooks/EcoScan/gradcam\"  # Puedes cambiar esto al directorio que prefieras\n",
        "\n",
        "# Extraer un batch de imágenes y etiquetas del generador\n",
        "X_test_sample, y_test_sample = next(test_data)\n",
        "\n",
        "# Asegurarte de que los datos de validación estén en el formato correcto\n",
        "validation_data = (X_test_sample, y_test_sample)\n",
        "\n",
        "# Crear un GradCAMCallback para cada clase (0 a 5, porque hay 6 clases en total)\n",
        "callbacks = []\n",
        "\n",
        "# Configurar un GradCAMCallback para cada clase\n",
        "for class_index in range(6):\n",
        "    callbacks.append(\n",
        "        GradCAMCallback(\n",
        "            validation_data=validation_data,\n",
        "            layer_name='block14_sepconv2_act',  # Asegúrate de que sea el nombre correcto en tu modelo Xception\n",
        "            class_index=class_index,\n",
        "            output_dir=os.path.join(output_dir, f\"class_{class_index}\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Callbacks\n",
        "early = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "# Añadir otros callbacks como EarlyStopping y ReduceLROnPlateau\n",
        "callbacks.extend([early, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet"
      ],
      "metadata": {
        "id": "6tHacjOyjzcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "import mlflow\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Configurar la URI de almacenamiento de MLflow\n",
        "local_registry = \"sqlite:///mlruns.db\"\n",
        "print(f\"Running local model registry={local_registry}\")\n",
        "mlflow.set_tracking_uri(local_registry)\n",
        "\n",
        "# Nombre del modelo que se registrará\n",
        "model_name = \"ClasificadorResiduos\"\n",
        "\n",
        "# Configura el nombre del experimento\n",
        "experiment_name = \"Clasificacion_Residuos_Proyecto\"\n",
        "mlflow.set_experiment(experiment_name)"
      ],
      "metadata": {
        "id": "DpG0Pu9tj0YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEQleGGa5-jy"
      },
      "outputs": [],
      "source": [
        "from mlflow import log_metric, log_param, log_artifacts\n",
        "import shutil\n",
        "\n",
        "with mlflow.start_run(run_name=\"Xception_Model_Training\") as run:\n",
        "    # Registrar los hiperparámetros\n",
        "    mlflow.log_param(\"learning_rate\", 0.0001)\n",
        "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    result = model.fit(\n",
        "        train_data,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=n_steps,\n",
        "        validation_data=test_data,\n",
        "        validation_steps=n_val_steps,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Registrar las métricas finales\n",
        "    final_accuracy = result.history['val_accuracy'][-1]\n",
        "    final_loss = result.history['val_loss'][-1]\n",
        "    mlflow.log_metric(\"val_accuracy\", final_accuracy)\n",
        "    mlflow.log_metric(\"val_loss\", final_loss)\n",
        "\n",
        "    # Registrar el modelo en MLflow\n",
        "    mlflow.keras.log_model(\n",
        "        model,\n",
        "        artifact_path=\"xception-model\",\n",
        "        registered_model_name=model_name\n",
        "    )\n",
        "\n",
        "    # Crear un archivo de salida de ejemplo\n",
        "    if not os.path.exists(\"outputs\"):\n",
        "        os.makedirs(\"outputs\")\n",
        "    with open(\"outputs/test.txt\", \"w\") as f:\n",
        "        f.write(\"El modelo fue registrado exitosamente en el almacén local de MLflow.\")\n",
        "    log_artifacts(\"outputs\")\n",
        "    shutil.rmtree('outputs')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar la interfaz de usuario de MLflow en segundo plano\n",
        "get_ipython().system_raw(\"mlflow ui --backend-store-uri sqlite:///content/mlruns.db --port 5000 &\")\n",
        "\n",
        "# Configura pyngrok para crear un túnel a la interfaz de MLflow\n",
        "ngrok.kill()  # Termina cualquier túnel abierto\n",
        "NGROK_AUTH_TOKEN = \"2oHZxkpk4odzZvvnGecEiF20nhb_5y5mVAyrwnP4FkKTP3juE\"  # Reemplaza con tu token de ngrok\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "ZFdUe9tJFsk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKUP-j6h-NgL"
      },
      "outputs": [],
      "source": [
        "# Gráfica de precisión (Accuracy)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(result.history[\"accuracy\"], label=\"Entrenamiento - Precisión\")\n",
        "plt.plot(result.history[\"val_accuracy\"], label=\"Validación - Precisión\")\n",
        "plt.title(\"Precisión del Modelo durante el Entrenamiento y Validación (Xception Final)\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Precisión\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Gráfica de pérdida (Loss)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(result.history[\"loss\"], label=\"Entrenamiento - Pérdida\")\n",
        "plt.plot(result.history[\"val_loss\"], label=\"Validación - Pérdida\")\n",
        "plt.title(\"Pérdida del Modelo durante el Entrenamiento y Validación (Xception Final)\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Pérdida\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Gráfico combinado de precisión y pérdida\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(result.history['accuracy'], label=\"Precisión - Entrenamiento\")\n",
        "plt.plot(result.history['val_accuracy'], label=\"Precisión - Validación\")\n",
        "plt.plot(result.history['loss'], label=\"Pérdida - Entrenamiento\")\n",
        "plt.plot(result.history['val_loss'], label=\"Pérdida - Validación\")\n",
        "plt.title(\"Precisión y Pérdida del Modelo (Xception Final)\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.ylabel(\"Precisión / Pérdida\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8JUmBPL-Xe2"
      },
      "outputs": [],
      "source": [
        "# Hacer predicciones con el modelo\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Obtener etiquetas predichas\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Etiquetas verdaderas\n",
        "true_labels = test_data.classes\n",
        "\n",
        "# Matriz de confusión\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Graficar matriz de confusión con seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=test_data.class_indices.keys(),\n",
        "            yticklabels=test_data.class_indices.keys())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Xception Final)')\n",
        "plt.show()\n",
        "\n",
        "# Reporte de clasificación\n",
        "class_names = list(test_data.class_indices.keys())\n",
        "print(classification_report(true_labels, predicted_labels, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49UkTqhf1IuD"
      },
      "source": [
        "### Saliency Maps para el mapa de calor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_-8p8rkE2rF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Función para calcular el mapa de saliencia para una clase específica usando SmoothGrad\n",
        "def smooth_grad(model, img_tensor, class_index, noise_level=0.1, num_samples=20):\n",
        "    \"\"\"Genera un mapa de saliencia usando SmoothGrad.\"\"\"\n",
        "    smooth_saliency = np.zeros(img_tensor.shape[:2])\n",
        "    for _ in range(num_samples):\n",
        "        noise = np.random.normal(0, noise_level, img_tensor.shape)\n",
        "        noisy_img = img_tensor + noise\n",
        "        saliency = compute_saliency_map(model, noisy_img, class_index)\n",
        "        smooth_saliency += saliency\n",
        "    smooth_saliency /= num_samples  # Promedio\n",
        "    return smooth_saliency\n",
        "\n",
        "# Función básica de cálculo de saliencia (utilizada dentro de SmoothGrad)\n",
        "def compute_saliency_map(model, img_tensor, label_index):\n",
        "    \"\"\"Calcula el mapa de saliencia para una imagen y una clase específica.\"\"\"\n",
        "    img_tensor = tf.convert_to_tensor([img_tensor])  # Agregar dimensión batch\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img_tensor)\n",
        "        preds = model(img_tensor)\n",
        "        loss = preds[:, label_index]\n",
        "    grads = tape.gradient(loss, img_tensor)\n",
        "    saliency = K.abs(grads)[0]\n",
        "    saliency = np.max(saliency, axis=-1)\n",
        "    return saliency\n",
        "\n",
        "# Mostrar mapas de saliencia usando SmoothGrad para cada clase\n",
        "class_labels = train_data.class_indices.keys()  # Asegúrate de tener acceso a los nombres de las clases\n",
        "\n",
        "for class_index, class_name in enumerate(class_labels):\n",
        "    # Obtener una imagen de muestra de la clase actual\n",
        "    for images, labels in train_data:\n",
        "        img_tensor = images[0]  # Primera imagen del lote\n",
        "        label = labels[0].argmax()  # Obtener el índice de la clase (sin usar .numpy())\n",
        "\n",
        "        # Asegurarse de que la imagen sea de la clase deseada\n",
        "        if label == class_index:\n",
        "            # Generar el mapa de saliencia usando SmoothGrad\n",
        "            smooth_saliency = smooth_grad(model, img_tensor, class_index)\n",
        "\n",
        "            # Normalizar la imagen para la visualización (escala [0, 1])\n",
        "            img_display = (img_tensor - img_tensor.min()) / (img_tensor.max() - img_tensor.min())\n",
        "\n",
        "            # Visualizar la imagen y el mapa de saliencia\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(img_display)\n",
        "            plt.title(f'Imagen original - Clase: {class_name}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.imshow(smooth_saliency, cmap='hot')\n",
        "            plt.title(f'SmoothGrad (Xception Final) - Clase: {class_name}')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            break  # Solo una imagen por clase"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP para Explainable AI"
      ],
      "metadata": {
        "id": "ZLqDfs0yTkv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "JUpDugFO8Dxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "from lime import lime_image\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import numpy as np\n",
        "\n",
        "# Crear un explicador LIME para imágenes\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# Diccionario para traducir las etiquetas al español\n",
        "class_translation = {\n",
        "    'battery': 'e-waste',\n",
        "    'glass': 'vidrio',\n",
        "    'metal': 'metal',\n",
        "    'organic': 'orgánico',\n",
        "    'paper': 'papel',\n",
        "    'plastic': 'plástico'\n",
        "}\n",
        "\n",
        "# Configuración: número de ejemplos por clase\n",
        "num_classes = 6\n",
        "examples_per_class = 5  # Puedes ajustar esto para ver más ejemplos por clase\n",
        "class_examples = {i: 0 for i in range(num_classes)}  # Contador de ejemplos por clase\n",
        "\n",
        "# Obtener un batch del conjunto de datos de validación\n",
        "X_test_sample, y_test_sample = next(test_data)\n",
        "\n",
        "# Recorrer las imágenes en el batch\n",
        "for i in range(len(X_test_sample)):\n",
        "    image = X_test_sample[i]\n",
        "    label = np.argmax(y_test_sample[i])  # Convertir la etiqueta one-hot a un valor entero\n",
        "\n",
        "    # Traducir la etiqueta al español\n",
        "    class_name = list(class_translation.values())[label]  # Obtener el nombre traducido\n",
        "\n",
        "    # Si aún no hemos generado suficientes ejemplos para esta clase\n",
        "    if class_examples[label] < examples_per_class:\n",
        "        # Explicar la predicción del modelo para esta imagen\n",
        "        explanation = explainer.explain_instance(\n",
        "            image,\n",
        "            model.predict,\n",
        "            top_labels=5,\n",
        "            hide_color=0,\n",
        "            num_samples=1000\n",
        "        )\n",
        "\n",
        "        # Obtener la imagen explicada y la máscara\n",
        "        temp, mask = explanation.get_image_and_mask(\n",
        "            label,\n",
        "            positive_only=True,\n",
        "            num_features=5,\n",
        "            hide_rest=False\n",
        "        )\n",
        "\n",
        "        # Mostrar la explicación visual con LIME\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(mark_boundaries(temp, mask))\n",
        "        plt.title(f\"Explicación con LIME - {class_name}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Incrementar el contador para esta clase\n",
        "        class_examples[label] += 1\n",
        "\n",
        "    # Terminar si hemos alcanzado el número deseado de ejemplos por clase\n",
        "    if all(count >= examples_per_class for count in class_examples.values()):\n",
        "        break"
      ],
      "metadata": {
        "id": "vzW9es468AXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adrzJGn9QUV4"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo final\n",
        "model.save('/content/drive/MyDrive/Colab_Notebooks/EcoScan/wastenet_xception_final.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UirETsFDhAA_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Path a las imágenes reales para prueba\n",
        "test_path = '/content/drive/MyDrive/Colab_Notebooks/EcoScan/test_real'\n",
        "\n",
        "# Obtener los nombres de las imágenes\n",
        "image_files = [os.path.join(test_path, img) for img in os.listdir(test_path)]\n",
        "\n",
        "# Mostrar todas las imágenes\n",
        "def display_images(image_paths):\n",
        "    plt.figure(figsize=(15, 15))  # Ajusta el tamaño de la visualización\n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        img = image.load_img(img_path)\n",
        "        plt.subplot(5, 5, i + 1)  # Crea una grilla 5x5 (ajústalo según la cantidad de imágenes)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Mostrar las imágenes del directorio\n",
        "display_images(image_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZnWnq6phwdM"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "model = load_model('/content/drive/MyDrive/Colab_Notebooks/EcoScan/wastenet_xception.h5')\n",
        "\n",
        "# Diccionario para traducir las etiquetas al español\n",
        "class_translation = {\n",
        "    'battery': 'e-waste',\n",
        "    'glass': 'vidrio',\n",
        "    'metal': 'metal',\n",
        "    'organic': 'orgánico',\n",
        "    'paper': 'papel',\n",
        "    'plastic': 'plástico'\n",
        "}\n",
        "\n",
        "# Lista para almacenar los tiempos de clasificación\n",
        "classification_times = []\n",
        "\n",
        "# Función para predecir la clase de una imagen y mostrarla junto con su etiqueta\n",
        "def classify_and_display_image(image_path):\n",
        "    # Cargar y mostrar la imagen\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Convertir la imagen a array y preprocesarla\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    # Tomar el tiempo de inicio\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Hacer la predicción\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    # Tomar el tiempo de fin y calcular el tiempo\n",
        "    end_time = time.time()\n",
        "    classification_time = end_time - start_time\n",
        "    classification_times.append(classification_time)\n",
        "\n",
        "    # Obtener la clase predicha (índice) y traducirla al español\n",
        "    predicted_class_idx = np.argmax(predictions[0])\n",
        "    predicted_class = list(class_translation.keys())[predicted_class_idx]\n",
        "    translated_class = class_translation[predicted_class]\n",
        "\n",
        "    # Mostrar la etiqueta traducida\n",
        "    plt.title(f'Clasificación (Xception Final): {translated_class}')\n",
        "    plt.show()\n",
        "\n",
        "# Clasificar y mostrar cada imagen\n",
        "for image_file in image_files:\n",
        "    classify_and_display_image(image_file)\n",
        "\n",
        "# Calcular el tiempo promedio de clasificación\n",
        "average_time = np.mean(classification_times)\n",
        "print(f'Tiempo promedio de clasificación por imagen (Xception Final): {average_time:.4f} segundos')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Función para calcular SmoothGrad\n",
        "def smooth_grad(model, img_tensor, class_index, noise_level=0.1, num_samples=20):\n",
        "    smooth_saliency = np.zeros(img_tensor.shape[:2])\n",
        "    for _ in range(num_samples):\n",
        "        noise = np.random.normal(0, noise_level, img_tensor.shape)\n",
        "        noisy_img = img_tensor + noise\n",
        "        saliency = compute_saliency_map(model, noisy_img, class_index)\n",
        "        smooth_saliency += saliency\n",
        "    smooth_saliency /= num_samples\n",
        "    return smooth_saliency\n",
        "\n",
        "# Función básica de cálculo de saliencia\n",
        "def compute_saliency_map(model, img_tensor, label_index):\n",
        "    img_tensor = tf.convert_to_tensor([img_tensor])\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img_tensor)\n",
        "        preds = model(img_tensor)\n",
        "        loss = preds[:, label_index]\n",
        "    grads = tape.gradient(loss, img_tensor)\n",
        "    saliency = K.abs(grads)[0]\n",
        "    saliency = np.max(saliency, axis=-1)\n",
        "    return saliency\n",
        "\n",
        "# Función para predecir, mostrar la clasificación y el mapa de saliencia de cada imagen\n",
        "def classify_and_display_with_saliency(image_path):\n",
        "    # Cargar y mostrar la imagen original\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = preprocess_input(np.expand_dims(img_array, axis=0))\n",
        "\n",
        "    # Realizar predicción\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class_idx = np.argmax(predictions[0])\n",
        "    predicted_class = list(class_translation.keys())[predicted_class_idx]\n",
        "    translated_class = class_translation[predicted_class]\n",
        "\n",
        "    # Calcular el mapa de saliencia usando SmoothGrad\n",
        "    smooth_saliency = smooth_grad(model, img_array[0], predicted_class_idx)\n",
        "\n",
        "    # Visualizar la imagen, el nombre de la clase y el mapa de saliencia\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Imagen original\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image.array_to_img(img_array[0] * 0.5 + 0.5))  # Escala de [-1,1] a [0,1]\n",
        "    plt.title(f'Clasificación (Xception Final): {translated_class}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Mapa de Saliencia\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(smooth_saliency, cmap='hot')\n",
        "    plt.title(f'SmoothGrad (Xception Final) - Clase: {translated_class}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Obtener nombres de las imágenes y aplicar la clasificación y saliencia\n",
        "image_files = [os.path.join(test_path, img) for img in os.listdir(test_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "for image_file in image_files:\n",
        "    classify_and_display_with_saliency(image_file)"
      ],
      "metadata": {
        "id": "2_kfGBfSSmIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar e iniciar TensorBoard en Google Colab\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/Colab_Notebooks/EcoScan/gradcam\""
      ],
      "metadata": {
        "id": "_FPbl1dyf-j-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}